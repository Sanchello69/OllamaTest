import kotlinx.coroutines.runBlocking
import java.io.File

fun main(args: Array<String>) = runBlocking {
    println("=== RTF to Vector Embeddings with Ollama ===\n")

    // Parse command line arguments
    if (args.isEmpty()) {
        printUsage()
        return@runBlocking
    }

    val command = args[0]

    when (command) {
        "index" -> {
            if (args.size < 2) {
                println("Error: Please provide RTF file path")
                println("Usage: index <rtf-file-path> [options]")
                return@runBlocking
            }

            val rtfFilePath = args[1]
            val indexPath = if (args.size > 2) args[2] else "embeddings_index.json"
            val maxChunkSize = if (args.size > 3) args[3].toIntOrNull() ?: 2000 else 2000
            val minChunkSize = if (args.size > 4) args[4].toIntOrNull() ?: 50 else 50

            indexRtfFile(rtfFilePath, indexPath, maxChunkSize, minChunkSize)
        }

        "search" -> {
            if (args.size < 3) {
                println("Error: Please provide index path and search query")
                println("Usage: search <index-path> <query> [top-k]")
                return@runBlocking
            }

            val indexPath = args[1]
            val query = args[2]
            val topK = if (args.size > 3) args[3].toIntOrNull() ?: 5 else 5

            searchIndex(indexPath, query, topK)
        }

        "stats" -> {
            if (args.size < 2) {
                println("Error: Please provide index path")
                println("Usage: stats <index-path>")
                return@runBlocking
            }

            val indexPath = args[1]
            showStats(indexPath)
        }

        "ask" -> {
            if (args.size < 2) {
                println("Error: Please provide question")
                println("Usage: ask <question> [index-path] [--no-rag] [--min-score=0.7] [--save-history] [--history-path=path]")
                return@runBlocking
            }

            val question = args[1]
            val useRag = !args.contains("--no-rag")
            val indexPath = if (args.size > 2 && !args[2].startsWith("--")) args[2] else "embeddings_index.json"

            // –ò–∑–≤–ª–µ–∫–∞–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ –ø–æ—Ö–æ–∂–µ—Å—Ç–∏ –∏–∑ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤
            val minScoreArg = args.find { it.startsWith("--min-score=") }
            val minRelevanceScore = minScoreArg?.substringAfter("=")?.toDoubleOrNull() ?: 0.0

            // –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∏—Å—Ç–æ—Ä–∏–∏
            val saveHistory = args.contains("--save-history")
            val historyPathArg = args.find { it.startsWith("--history-path=") }
            val historyPath = historyPathArg?.substringAfter("=") ?: "conversation_history.json"

            // –ó–∞–≥—Ä—É–∑–∫–∞ –∏—Å—Ç–æ—Ä–∏–∏, –µ—Å–ª–∏ —Ñ–∞–π–ª —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
            val history = ConversationHistory()
            if (saveHistory && File(historyPath).exists()) {
                try {
                    history.load(historyPath)
                } catch (e: Exception) {
                    println("‚ö†Ô∏è  Could not load history: ${e.message}")
                }
            }

            askQuestion(question, indexPath, useRag, minRelevanceScore, history, saveHistory, historyPath)
        }

        "chat" -> {
            if (args.size < 1) {
                println("Error: Please use chat command")
                println("Usage: chat [index-path] [--no-rag] [--min-score=0.7] [--history-path=path]")
                return@runBlocking
            }

            val useRag = !args.contains("--no-rag")
            val indexPath = if (args.size > 1 && !args[1].startsWith("--")) args[1] else "embeddings_index.json"

            // –ò–∑–≤–ª–µ–∫–∞–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ –ø–æ—Ö–æ–∂–µ—Å—Ç–∏ –∏–∑ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤
            val minScoreArg = args.find { it.startsWith("--min-score=") }
            val minRelevanceScore = minScoreArg?.substringAfter("=")?.toDoubleOrNull() ?: 0.0

            // –ü—É—Ç—å –∫ –∏—Å—Ç–æ—Ä–∏–∏
            val historyPathArg = args.find { it.startsWith("--history-path=") }
            val historyPath = historyPathArg?.substringAfter("=") ?: "conversation_history.json"

            startChat(indexPath, useRag, minRelevanceScore, historyPath)
        }

        else -> {
            println("Unknown command: $command")
            printUsage()
        }
    }
}

suspend fun indexRtfFile(rtfFilePath: String, indexPath: String, maxChunkSize: Int, minChunkSize: Int) {
    println("üìÑ Processing RTF file: $rtfFilePath")
    println("Settings:")
    println("  - Chunking mode: –ü–æ –∞–±–∑–∞—Ü–∞–º")
    println("  - Max chunk size: $maxChunkSize characters")
    println("  - Min chunk size: $minChunkSize characters")
    println("  - Index output: $indexPath\n")

    // Step 1: Parse RTF file
    println("Step 1: Parsing RTF file...")
    val parser = RtfParser()
    val text = try {
        parser.parseRtfFile(rtfFilePath)
    } catch (e: Exception) {
        println("‚ùå Error parsing RTF file: ${e.message}")
        return
    }
    println("‚úì Extracted ${text.length} characters\n")

    // Step 2: Split into chunks
    println("Step 2: Splitting text into chunks by paragraphs...")
    val chunker = TextChunker(maxChunkSize, minChunkSize)
    val chunks = chunker.chunkText(text)
    println("‚úì Created ${chunks.size} chunks\n")

    if (chunks.isEmpty()) {
        println("‚ùå No chunks created. The file might be empty.")
        return
    }

    // Step 3: Generate embeddings
    println("Step 3: Generating embeddings with Ollama...")
    println("Note: Make sure Ollama is running with the 'nomic-embed-text' model")
    println("Run: ollama pull nomic-embed-text\n")

    val ollamaClient = OllamaClient()

    val chunkTexts = chunks.map { it.text }
    val embeddings = try {
        ollamaClient.generateEmbeddings(chunkTexts) { current, total ->
            print("\rProgress: $current/$total chunks processed")
        }
    } catch (e: Exception) {
        println("\n‚ùå Error generating embeddings: ${e.message}")
        println("Make sure Ollama is running: http://localhost:11434")
        ollamaClient.close()
        return
    } finally {
        ollamaClient.close()
    }

    println("\n‚úì Generated ${embeddings.size} embeddings\n")

    // Step 4: Create and save index
    println("Step 4: Creating vector index...")
    val index = VectorIndex()

    chunks.forEachIndexed { idx, chunk ->
        val metadata = mapOf(
            "chunk_index" to idx.toString(),
            "start_pos" to chunk.startPosition.toString(),
            "end_pos" to chunk.endPosition.toString(),
            "source_file" to rtfFilePath
        )
        index.add(chunk.text, embeddings[idx], metadata)
    }

    println("‚úì Index created with ${index.size()} entries\n")

    // Step 5: Save index
    println("Step 5: Saving index to disk...")
    try {
        index.save(indexPath)
        println("‚úì Index saved successfully\n")
        println(index.getStats())
        println("\n‚úÖ Processing complete!")
    } catch (e: Exception) {
        println("‚ùå Error saving index: ${e.message}")
    }
}

suspend fun searchIndex(indexPath: String, query: String, topK: Int) {
    println("üîç Searching index: $indexPath")
    println("Query: \"$query\"")
    println("Top K results: $topK\n")

    // Load index
    println("Loading index...")
    val index = VectorIndex()
    try {
        index.load(indexPath)
    } catch (e: Exception) {
        println("‚ùå Error loading index: ${e.message}")
        return
    }

    // Generate embedding for query
    println("Generating query embedding...")
    val ollamaClient = OllamaClient()
    val queryEmbedding = try {
        ollamaClient.generateEmbedding(query)
    } catch (e: Exception) {
        println("‚ùå Error generating query embedding: ${e.message}")
        ollamaClient.close()
        return
    } finally {
        ollamaClient.close()
    }

    // Search
    println("Searching...\n")
    val results = index.search(queryEmbedding, topK)

    if (results.isEmpty()) {
        println("No results found.")
        return
    }

    println("Results:\n")
    results.forEachIndexed { idx, result ->
        println("${idx + 1}. Score: ${"%.4f".format(result.score)}")
        println("   Text: ${result.text.take(200)}${if (result.text.length > 200) "..." else ""}")
        println("   Metadata: ${result.metadata}")
        println()
    }
}

fun showStats(indexPath: String) {
    println("üìä Index Statistics\n")

    val index = VectorIndex()
    try {
        index.load(indexPath)
        println(index.getStats())
    } catch (e: Exception) {
        println("‚ùå Error loading index: ${e.message}")
    }
}

suspend fun askQuestion(
    question: String,
    indexPath: String,
    useRag: Boolean,
    minRelevanceScore: Double = 0.0,
    conversationHistory: ConversationHistory? = null,
    saveHistory: Boolean = false,
    historyPath: String = "conversation_history.json"
) {
    println("ü§ñ AI Assistant ${if (useRag) "with RAG" else "without RAG"}\n")
    println("Question: \"$question\"")
    if (useRag && minRelevanceScore > 0.0) {
        println("üîç Relevance filter: minimum score = ${"%.2f".format(minRelevanceScore)}")
    }
    if (conversationHistory != null && conversationHistory.size() > 0) {
        println("üìú –ò—Å—Ç–æ—Ä–∏—è –¥–∏–∞–ª–æ–≥–∞: ${conversationHistory.size()} —Ä–∞—É–Ω–¥–æ–≤")
    }
    println()

    val openRouterClient = OpenRouterClient()
    val history = conversationHistory ?: ConversationHistory()

    try {
        if (useRag) {
            // RAG —Ä–µ–∂–∏–º: –ø–æ–∏—Å–∫ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ + –æ—Ç–≤–µ—Ç
            println("Step 1: Loading index and searching for relevant context...")

            val index = VectorIndex()
            try {
                index.load(indexPath)
                println("‚úì Index loaded (${index.size()} entries)\n")
            } catch (e: Exception) {
                println("‚ùå Error loading index: ${e.message}")
                println("Falling back to non-RAG mode...\n")
                val answer = if (history.size() > 0) {
                    openRouterClient.askQuestionWithHistory(question, history.getMessagesForLLM())
                } else {
                    openRouterClient.askQuestion(question)
                }
                println("üí¨ Answer:\n$answer")

                // –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∏—Å—Ç–æ—Ä–∏—é –±–µ–∑ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
                history.addTurn(question, answer, emptyList(), useRag = false)
                if (saveHistory) history.save(historyPath)

                println(history.formatSources(emptyList()))
                return
            }

            // –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è –≤–æ–ø—Ä–æ—Å–∞
            println("Step 2: Generating question embedding...")
            val ollamaClient = OllamaClient()
            val queryEmbedding = try {
                ollamaClient.generateEmbedding(question)
            } catch (e: Exception) {
                println("‚ùå Error generating query embedding: ${e.message}")
                ollamaClient.close()
                println("Falling back to non-RAG mode...\n")
                val answer = if (history.size() > 0) {
                    openRouterClient.askQuestionWithHistory(question, history.getMessagesForLLM())
                } else {
                    openRouterClient.askQuestion(question)
                }
                println("üí¨ Answer:\n$answer")

                // –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∏—Å—Ç–æ—Ä–∏—é –±–µ–∑ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
                history.addTurn(question, answer, emptyList(), useRag = false)
                if (saveHistory) history.save(historyPath)

                println(history.formatSources(emptyList()))
                return
            } finally {
                ollamaClient.close()
            }
            println("‚úì Embedding generated\n")

            // –ò—â–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —á–∞–Ω–∫–∏
            println("Step 3: Searching for relevant chunks...")
            val allResults = index.search(queryEmbedding, 5)

            // –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –ø–æ—Ä–æ–≥—É —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
            val results = if (minRelevanceScore > 0.0) {
                allResults.filter { it.score >= minRelevanceScore }
            } else {
                allResults
            }

            if (results.isEmpty()) {
                if (minRelevanceScore > 0.0) {
                    println("‚ö†Ô∏è  No chunks found with score >= ${"%.2f".format(minRelevanceScore)}")
                    println("   Top result score was: ${"%.4f".format(allResults.firstOrNull()?.score ?: 0.0)}")
                } else {
                    println("‚ö†Ô∏è  No relevant chunks found")
                }
                println("Falling back to non-RAG mode...\n")
                val answer = if (history.size() > 0) {
                    openRouterClient.askQuestionWithHistory(question, history.getMessagesForLLM())
                } else {
                    openRouterClient.askQuestion(question)
                }
                println("üí¨ Answer:\n$answer")

                // –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∏—Å—Ç–æ—Ä–∏—é –±–µ–∑ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
                history.addTurn(question, answer, emptyList(), useRag = false)
                if (saveHistory) history.save(historyPath)

                println(history.formatSources(emptyList()))
                return
            }

            println("‚úì Found ${results.size} relevant chunks")
            if (minRelevanceScore > 0.0 && results.size < allResults.size) {
                println("   (filtered ${allResults.size - results.size} chunks below threshold)")
            }
            println()

            // –û—Ç–æ–±—Ä–∞–∂–∞–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —á–∞–Ω–∫–∏
            println("üìö Relevant chunks:")
            results.forEachIndexed { idx, result ->
                println("  ${idx + 1}. [Score: ${"%.4f".format(result.score)}] ${result.text.take(100)}...")
            }
            println()

            // –û–±—ä–µ–¥–∏–Ω—è–µ–º —á–∞–Ω–∫–∏ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç
            val context = results.joinToString("\n\n---\n\n") { it.text }

            // –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –≤–æ–ø—Ä–æ—Å —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –≤ LLM (—Å —É—á–µ—Ç–æ–º –∏—Å—Ç–æ—Ä–∏–∏)
            println("Step 4: Sending question with context to LLM...")
            val answer = if (history.size() > 0) {
                openRouterClient.askQuestionWithHistory(question, history.getMessagesForLLM(), context)
            } else {
                openRouterClient.askQuestion(question, context)
            }

            println("‚úì Response received\n")
            println("üí¨ Answer:\n$answer")

            // –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∏—Å—Ç–æ—Ä–∏—é —Å –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏
            history.addTurn(question, answer, results, useRag = true)
            if (saveHistory) {
                history.save(historyPath)
            }

            // –í—ã–≤–æ–¥–∏–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏
            println(history.formatSources(history.getAllTurns().last().sources))

        } else {
            // –ë–µ–∑ RAG: –ø—Ä–æ—Å—Ç–æ –≤–æ–ø—Ä–æ—Å –∫ LLM
            println("Sending question to LLM (without context)...")
            val answer = if (history.size() > 0) {
                openRouterClient.askQuestionWithHistory(question, history.getMessagesForLLM())
            } else {
                openRouterClient.askQuestion(question)
            }

            println("‚úì Response received\n")
            println("üí¨ Answer:\n$answer")

            // –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∏—Å—Ç–æ—Ä–∏—é –±–µ–∑ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
            history.addTurn(question, answer, emptyList(), useRag = false)
            if (saveHistory) {
                history.save(historyPath)
            }

            // –í—ã–≤–æ–¥–∏–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏ (–∏—Ö –Ω–µ—Ç)
            println(history.formatSources(emptyList()))
        }
    } catch (e: Exception) {
        println("‚ùå Error: ${e.message}")
        e.printStackTrace()
    } finally {
        openRouterClient.close()
    }
}

suspend fun startChat(
    indexPath: String,
    useRag: Boolean,
    minRelevanceScore: Double = 0.0,
    historyPath: String = "conversation_history.json"
) {
    println("üí¨ –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —á–∞—Ç ${if (useRag) "—Å RAG" else "–±–µ–∑ RAG"}")
    println("üìù –ò—Å—Ç–æ—Ä–∏—è —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è –≤: $historyPath")
    if (useRag && minRelevanceScore > 0.0) {
        println("üîç –§–∏–ª—å—Ç—Ä —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏: >= ${"%.2f".format(minRelevanceScore)}")
    }
    println("\n–ö–æ–º–∞–Ω–¥—ã:")
    println("  - –í–≤–µ–¥–∏—Ç–µ –≤–æ–ø—Ä–æ—Å –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –æ—Ç–≤–µ—Ç–∞")
    println("  - /history - –ø–æ–∫–∞–∑–∞—Ç—å –∏—Å—Ç–æ—Ä–∏—é –¥–∏–∞–ª–æ–≥–∞")
    println("  - /stats - –ø–æ–∫–∞–∑–∞—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É")
    println("  - /clear - –æ—á–∏—Å—Ç–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é")
    println("  - /exit –∏–ª–∏ /quit - –≤—ã—Ö–æ–¥\n")

    // –ó–∞–≥—Ä—É–∑–∫–∞ –∏–ª–∏ —Å–æ–∑–¥–∞–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏
    val history = ConversationHistory()
    if (File(historyPath).exists()) {
        try {
            history.load(historyPath)
            println("‚úì –ó–∞–≥—Ä—É–∂–µ–Ω–∞ –∏—Å—Ç–æ—Ä–∏—è: ${history.size()} —Ä–∞—É–Ω–¥–æ–≤\n")
        } catch (e: Exception) {
            println("‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é: ${e.message}")
            println("–°–æ–∑–¥–∞–Ω–∞ –Ω–æ–≤–∞—è –∏—Å—Ç–æ—Ä–∏—è\n")
        }
    } else {
        println("–°–æ–∑–¥–∞–Ω–∞ –Ω–æ–≤–∞—è –∏—Å—Ç–æ—Ä–∏—è\n")
    }

    // –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª —á–∞—Ç–∞
    while (true) {
        print("–í—ã: ")
        val input = readLine()?.trim() ?: break

        if (input.isEmpty()) continue

        when (input.lowercase()) {
            "/exit", "/quit" -> {
                println("\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏...")
                history.save(historyPath)
                println("–î–æ —Å–≤–∏–¥–∞–Ω–∏—è!")
                break
            }

            "/history" -> {
                println("\nüìú –ò—Å—Ç–æ—Ä–∏—è –¥–∏–∞–ª–æ–≥–∞:")
                if (history.size() == 0) {
                    println("  –ò—Å—Ç–æ—Ä–∏—è –ø—É—Å—Ç–∞")
                } else {
                    history.getAllTurns().forEachIndexed { idx, turn ->
                        println("\n--- –†–∞—É–Ω–¥ ${idx + 1} [${turn.timestamp}] ---")
                        println("–í—ã: ${turn.question}")
                        println("AI: ${turn.answer.take(200)}${if (turn.answer.length > 200) "..." else ""}")
                        if (turn.sources.isNotEmpty()) {
                            println("–ò—Å—Ç–æ—á–Ω–∏–∫–æ–≤: ${turn.sources.size}")
                        }
                    }
                }
                println()
                continue
            }

            "/stats" -> {
                println("\n${history.getStats()}\n")
                continue
            }

            "/clear" -> {
                history.clear()
                println("\n‚úì –ò—Å—Ç–æ—Ä–∏—è –æ—á–∏—â–µ–Ω–∞\n")
                continue
            }

            else -> {
                // –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤–æ–ø—Ä–æ—Å
                println()
                askQuestion(input, indexPath, useRag, minRelevanceScore, history, saveHistory = true, historyPath)
                println()
            }
        }
    }
}

fun printUsage() {
    println("""
        Usage: java -jar OllamaTest.jar <command> [options]

        Commands:
          index <rtf-file> [index-path] [max-chunk-size] [min-chunk-size]
              Process RTF file and create embeddings index
              –¢–µ–∫—Å—Ç —Ä–∞–∑–±–∏–≤–∞–µ—Ç—Å—è –ø–æ –∞–±–∑–∞—Ü–∞–º, –∞ –Ω–µ –ø–æ —Å–∏–º–≤–æ–ª–∞–º

              Arguments:
                rtf-file         - Path to RTF file to process
                index-path       - Output path for index (default: embeddings_index.json)
                max-chunk-size   - Max size for large paragraphs (default: 2000)
                min-chunk-size   - Min size to filter small paragraphs (default: 50)

              Example:
                index document.rtf my_index.json 2000 50

          search <index-path> <query> [top-k]
              Search the index with a query

              Arguments:
                index-path   - Path to the index file
                query        - Search query text
                top-k        - Number of top results to return (default: 5)

              Example:
                search my_index.json "machine learning" 10

          ask <question> [index-path] [--no-rag] [--min-score=THRESHOLD] [--save-history] [--history-path=PATH]
              Ask a question to AI assistant (with or without RAG)

              Arguments:
                question           - Your question
                index-path         - Path to the index file (default: embeddings_index.json)
                --no-rag           - Disable RAG mode (no context retrieval)
                --min-score=X.X    - Minimum relevance score threshold (0.0-1.0, default: 0.0)
                --save-history     - Save conversation history
                --history-path=PATH - Path to history file (default: conversation_history.json)

              Examples:
                ask "–ö–∞–∫ –∑–≤–∞–ª–∏ —Å—Ç–µ–ø–Ω–æ–≥–æ –≤–æ–ª–∫–∞?"                    # With RAG
                ask "–ß—Ç–æ —Ç–∞–∫–æ–µ –º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ?" --no-rag       # Without RAG
                ask "–ö—Ç–æ –≥–ª–∞–≤–Ω—ã–π –≥–µ—Ä–æ–π?" my_index.json            # Custom index
                ask "–î–µ—Ç–∞–ª–∏ —Å—é–∂–µ—Ç–∞?" --min-score=0.75             # High relevance only
                ask "–ü—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ?" --save-history                 # Save to history

              RAG Mode (default):
                1. Finds relevant chunks from the index
                2. Filters by relevance score (if --min-score specified)
                3. Combines them with your question
                4. Sends to LLM for answer
                5. Shows sources used for the answer

              Relevance Filter (--min-score):
                - 0.0-0.5: Very loose (includes marginally relevant chunks)
                - 0.5-0.7: Moderate filtering (recommended for general use)
                - 0.7-0.9: Strict filtering (only highly relevant chunks)
                - 0.9-1.0: Very strict (almost exact matches only)

              Without RAG (--no-rag):
                Sends question directly to LLM without context

          chat [index-path] [--no-rag] [--min-score=THRESHOLD] [--history-path=PATH]
              Start interactive chat session with AI assistant

              Arguments:
                index-path         - Path to the index file (default: embeddings_index.json)
                --no-rag           - Disable RAG mode (no context retrieval)
                --min-score=X.X    - Minimum relevance score threshold (0.0-1.0, default: 0.0)
                --history-path=PATH - Path to history file (default: conversation_history.json)

              Interactive commands:
                <question>   - Ask a question
                /history     - Show conversation history
                /stats       - Show statistics
                /clear       - Clear history
                /exit, /quit - Exit chat

              Examples:
                chat                              # Start chat with RAG
                chat --no-rag                     # Start chat without RAG
                chat my_index.json --min-score=0.7  # Custom index with filter

              Features:
                - Maintains conversation context across questions
                - Automatically saves history after each question
                - Shows sources for each answer (with RAG)
                - Loads previous history on startup if exists

          stats <index-path>
              Show statistics about the index

              Example:
                stats my_index.json

        Prerequisites:
          - Ollama must be running (http://localhost:11434) for embeddings
          - Install embedding model: ollama pull nomic-embed-text
          - OpenRouter API key configured in OpenRouterClient.kt
    """.trimIndent())
}
