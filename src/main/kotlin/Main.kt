import kotlinx.coroutines.runBlocking
import java.io.File

fun main(args: Array<String>) = runBlocking {
    println("=== RTF to Vector Embeddings with Ollama ===\n")

    // Parse command line arguments
    if (args.isEmpty()) {
        printUsage()
        return@runBlocking
    }

    val command = args[0]

    when (command) {
        "index" -> {
            if (args.size < 2) {
                println("Error: Please provide RTF file path")
                println("Usage: index <rtf-file-path> [options]")
                return@runBlocking
            }

            val rtfFilePath = args[1]
            val indexPath = if (args.size > 2) args[2] else "embeddings_index.json"
            val maxChunkSize = if (args.size > 3) args[3].toIntOrNull() ?: 2000 else 2000
            val minChunkSize = if (args.size > 4) args[4].toIntOrNull() ?: 50 else 50

            indexRtfFile(rtfFilePath, indexPath, maxChunkSize, minChunkSize)
        }

        "search" -> {
            if (args.size < 3) {
                println("Error: Please provide index path and search query")
                println("Usage: search <index-path> <query> [top-k]")
                return@runBlocking
            }

            val indexPath = args[1]
            val query = args[2]
            val topK = if (args.size > 3) args[3].toIntOrNull() ?: 5 else 5

            searchIndex(indexPath, query, topK)
        }

        "stats" -> {
            if (args.size < 2) {
                println("Error: Please provide index path")
                println("Usage: stats <index-path>")
                return@runBlocking
            }

            val indexPath = args[1]
            showStats(indexPath)
        }

        "ask" -> {
            if (args.size < 2) {
                println("Error: Please provide question")
                println("Usage: ask <question> [index-path] [--no-rag] [--min-score=0.7]")
                return@runBlocking
            }

            val question = args[1]
            val useRag = !args.contains("--no-rag")
            val indexPath = if (args.size > 2 && !args[2].startsWith("--")) args[2] else "embeddings_index.json"

            // –ò–∑–≤–ª–µ–∫–∞–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ –ø–æ—Ö–æ–∂–µ—Å—Ç–∏ –∏–∑ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤
            val minScoreArg = args.find { it.startsWith("--min-score=") }
            val minRelevanceScore = minScoreArg?.substringAfter("=")?.toDoubleOrNull() ?: 0.0

            askQuestion(question, indexPath, useRag, minRelevanceScore)
        }

        else -> {
            println("Unknown command: $command")
            printUsage()
        }
    }
}

suspend fun indexRtfFile(rtfFilePath: String, indexPath: String, maxChunkSize: Int, minChunkSize: Int) {
    println("üìÑ Processing RTF file: $rtfFilePath")
    println("Settings:")
    println("  - Chunking mode: –ü–æ –∞–±–∑–∞—Ü–∞–º")
    println("  - Max chunk size: $maxChunkSize characters")
    println("  - Min chunk size: $minChunkSize characters")
    println("  - Index output: $indexPath\n")

    // Step 1: Parse RTF file
    println("Step 1: Parsing RTF file...")
    val parser = RtfParser()
    val text = try {
        parser.parseRtfFile(rtfFilePath)
    } catch (e: Exception) {
        println("‚ùå Error parsing RTF file: ${e.message}")
        return
    }
    println("‚úì Extracted ${text.length} characters\n")

    // Step 2: Split into chunks
    println("Step 2: Splitting text into chunks by paragraphs...")
    val chunker = TextChunker(maxChunkSize, minChunkSize)
    val chunks = chunker.chunkText(text)
    println("‚úì Created ${chunks.size} chunks\n")

    if (chunks.isEmpty()) {
        println("‚ùå No chunks created. The file might be empty.")
        return
    }

    // Step 3: Generate embeddings
    println("Step 3: Generating embeddings with Ollama...")
    println("Note: Make sure Ollama is running with the 'nomic-embed-text' model")
    println("Run: ollama pull nomic-embed-text\n")

    val ollamaClient = OllamaClient()

    val chunkTexts = chunks.map { it.text }
    val embeddings = try {
        ollamaClient.generateEmbeddings(chunkTexts) { current, total ->
            print("\rProgress: $current/$total chunks processed")
        }
    } catch (e: Exception) {
        println("\n‚ùå Error generating embeddings: ${e.message}")
        println("Make sure Ollama is running: http://localhost:11434")
        ollamaClient.close()
        return
    } finally {
        ollamaClient.close()
    }

    println("\n‚úì Generated ${embeddings.size} embeddings\n")

    // Step 4: Create and save index
    println("Step 4: Creating vector index...")
    val index = VectorIndex()

    chunks.forEachIndexed { idx, chunk ->
        val metadata = mapOf(
            "chunk_index" to idx.toString(),
            "start_pos" to chunk.startPosition.toString(),
            "end_pos" to chunk.endPosition.toString(),
            "source_file" to rtfFilePath
        )
        index.add(chunk.text, embeddings[idx], metadata)
    }

    println("‚úì Index created with ${index.size()} entries\n")

    // Step 5: Save index
    println("Step 5: Saving index to disk...")
    try {
        index.save(indexPath)
        println("‚úì Index saved successfully\n")
        println(index.getStats())
        println("\n‚úÖ Processing complete!")
    } catch (e: Exception) {
        println("‚ùå Error saving index: ${e.message}")
    }
}

suspend fun searchIndex(indexPath: String, query: String, topK: Int) {
    println("üîç Searching index: $indexPath")
    println("Query: \"$query\"")
    println("Top K results: $topK\n")

    // Load index
    println("Loading index...")
    val index = VectorIndex()
    try {
        index.load(indexPath)
    } catch (e: Exception) {
        println("‚ùå Error loading index: ${e.message}")
        return
    }

    // Generate embedding for query
    println("Generating query embedding...")
    val ollamaClient = OllamaClient()
    val queryEmbedding = try {
        ollamaClient.generateEmbedding(query)
    } catch (e: Exception) {
        println("‚ùå Error generating query embedding: ${e.message}")
        ollamaClient.close()
        return
    } finally {
        ollamaClient.close()
    }

    // Search
    println("Searching...\n")
    val results = index.search(queryEmbedding, topK)

    if (results.isEmpty()) {
        println("No results found.")
        return
    }

    println("Results:\n")
    results.forEachIndexed { idx, result ->
        println("${idx + 1}. Score: ${"%.4f".format(result.score)}")
        println("   Text: ${result.text.take(200)}${if (result.text.length > 200) "..." else ""}")
        println("   Metadata: ${result.metadata}")
        println()
    }
}

fun showStats(indexPath: String) {
    println("üìä Index Statistics\n")

    val index = VectorIndex()
    try {
        index.load(indexPath)
        println(index.getStats())
    } catch (e: Exception) {
        println("‚ùå Error loading index: ${e.message}")
    }
}

suspend fun askQuestion(question: String, indexPath: String, useRag: Boolean, minRelevanceScore: Double = 0.0) {
    println("ü§ñ AI Assistant ${if (useRag) "with RAG" else "without RAG"}\n")
    println("Question: \"$question\"")
    if (useRag && minRelevanceScore > 0.0) {
        println("üîç Relevance filter: minimum score = ${"%.2f".format(minRelevanceScore)}")
    }
    println()

    val openRouterClient = OpenRouterClient()

    try {
        if (useRag) {
            // RAG —Ä–µ–∂–∏–º: –ø–æ–∏—Å–∫ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ + –æ—Ç–≤–µ—Ç
            println("Step 1: Loading index and searching for relevant context...")

            val index = VectorIndex()
            try {
                index.load(indexPath)
                println("‚úì Index loaded (${index.size()} entries)\n")
            } catch (e: Exception) {
                println("‚ùå Error loading index: ${e.message}")
                println("Falling back to non-RAG mode...\n")
                val answer = openRouterClient.askQuestion(question)
                println("üí¨ Answer:\n$answer\n")
                return
            }

            // –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è –≤–æ–ø—Ä–æ—Å–∞
            println("Step 2: Generating question embedding...")
            val ollamaClient = OllamaClient()
            val queryEmbedding = try {
                ollamaClient.generateEmbedding(question)
            } catch (e: Exception) {
                println("‚ùå Error generating query embedding: ${e.message}")
                ollamaClient.close()
                println("Falling back to non-RAG mode...\n")
                val answer = openRouterClient.askQuestion(question)
                println("üí¨ Answer:\n$answer\n")
                return
            } finally {
                ollamaClient.close()
            }
            println("‚úì Embedding generated\n")

            // –ò—â–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —á–∞–Ω–∫–∏
            println("Step 3: Searching for relevant chunks...")
            val allResults = index.search(queryEmbedding, 5)

            // –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –ø–æ—Ä–æ–≥—É —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
            val results = if (minRelevanceScore > 0.0) {
                allResults.filter { it.score >= minRelevanceScore }
            } else {
                allResults
            }

            if (results.isEmpty()) {
                if (minRelevanceScore > 0.0) {
                    println("‚ö†Ô∏è  No chunks found with score >= ${"%.2f".format(minRelevanceScore)}")
                    println("   Top result score was: ${"%.4f".format(allResults.firstOrNull()?.score ?: 0.0)}")
                } else {
                    println("‚ö†Ô∏è  No relevant chunks found")
                }
                println("Falling back to non-RAG mode...\n")
                val answer = openRouterClient.askQuestion(question)
                println("üí¨ Answer:\n$answer\n")
                return
            }

            println("‚úì Found ${results.size} relevant chunks")
            if (minRelevanceScore > 0.0 && results.size < allResults.size) {
                println("   (filtered ${allResults.size - results.size} chunks below threshold)")
            }
            println()

            // –û—Ç–æ–±—Ä–∞–∂–∞–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —á–∞–Ω–∫–∏
            println("üìö Relevant chunks:")
            results.forEachIndexed { idx, result ->
                println("  ${idx + 1}. [Score: ${"%.4f".format(result.score)}] ${result.text.take(100)}...")
            }
            println()

            // –û–±—ä–µ–¥–∏–Ω—è–µ–º —á–∞–Ω–∫–∏ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç
            val context = results.joinToString("\n\n---\n\n") { it.text }

            // –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –≤–æ–ø—Ä–æ—Å —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –≤ LLM
            println("Step 4: Sending question with context to LLM...")
            val answer = openRouterClient.askQuestion(question, context)

            println("‚úì Response received\n")
            println("üí¨ Answer:\n$answer\n")

        } else {
            // –ë–µ–∑ RAG: –ø—Ä–æ—Å—Ç–æ –≤–æ–ø—Ä–æ—Å –∫ LLM
            println("Sending question to LLM (without context)...")
            val answer = openRouterClient.askQuestion(question)

            println("‚úì Response received\n")
            println("üí¨ Answer:\n$answer\n")
        }
    } catch (e: Exception) {
        println("‚ùå Error: ${e.message}")
        e.printStackTrace()
    } finally {
        openRouterClient.close()
    }
}

fun printUsage() {
    println("""
        Usage: java -jar OllamaTest.jar <command> [options]

        Commands:
          index <rtf-file> [index-path] [max-chunk-size] [min-chunk-size]
              Process RTF file and create embeddings index
              –¢–µ–∫—Å—Ç —Ä–∞–∑–±–∏–≤–∞–µ—Ç—Å—è –ø–æ –∞–±–∑–∞—Ü–∞–º, –∞ –Ω–µ –ø–æ —Å–∏–º–≤–æ–ª–∞–º

              Arguments:
                rtf-file         - Path to RTF file to process
                index-path       - Output path for index (default: embeddings_index.json)
                max-chunk-size   - Max size for large paragraphs (default: 2000)
                min-chunk-size   - Min size to filter small paragraphs (default: 50)

              Example:
                index document.rtf my_index.json 2000 50

          search <index-path> <query> [top-k]
              Search the index with a query

              Arguments:
                index-path   - Path to the index file
                query        - Search query text
                top-k        - Number of top results to return (default: 5)

              Example:
                search my_index.json "machine learning" 10

          ask <question> [index-path] [--no-rag] [--min-score=THRESHOLD]
              Ask a question to AI assistant (with or without RAG)

              Arguments:
                question         - Your question
                index-path       - Path to the index file (default: embeddings_index.json)
                --no-rag         - Disable RAG mode (no context retrieval)
                --min-score=X.X  - Minimum relevance score threshold (0.0-1.0, default: 0.0)

              Examples:
                ask "–ö–∞–∫ –∑–≤–∞–ª–∏ —Å—Ç–µ–ø–Ω–æ–≥–æ –≤–æ–ª–∫–∞?"                    # With RAG
                ask "–ß—Ç–æ —Ç–∞–∫–æ–µ –º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ?" --no-rag       # Without RAG
                ask "–ö—Ç–æ –≥–ª–∞–≤–Ω—ã–π –≥–µ—Ä–æ–π?" my_index.json            # Custom index
                ask "–î–µ—Ç–∞–ª–∏ —Å—é–∂–µ—Ç–∞?" --min-score=0.75             # High relevance only

              RAG Mode (default):
                1. Finds relevant chunks from the index
                2. Filters by relevance score (if --min-score specified)
                3. Combines them with your question
                4. Sends to LLM for answer

              Relevance Filter (--min-score):
                - 0.0-0.5: Very loose (includes marginally relevant chunks)
                - 0.5-0.7: Moderate filtering (recommended for general use)
                - 0.7-0.9: Strict filtering (only highly relevant chunks)
                - 0.9-1.0: Very strict (almost exact matches only)

              Without RAG (--no-rag):
                Sends question directly to LLM without context

          stats <index-path>
              Show statistics about the index

              Example:
                stats my_index.json

        Prerequisites:
          - Ollama must be running (http://localhost:11434) for embeddings
          - Install embedding model: ollama pull nomic-embed-text
          - OpenRouter API key configured in OpenRouterClient.kt
    """.trimIndent())
}
